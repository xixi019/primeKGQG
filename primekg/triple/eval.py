import evaluate
from transformers import pipeline
import json
from statistics import mean
from gpt_baseline_1 import call_openAI
from mistral.mistral_baseline import call_bioLLM, call_mistral
from SPARQLWrapper import SPARQLWrapper, JSON
import time
from transformers.utils import logging
from util import form_prompt_gpt

logging.set_verbosity_info()
logger = logging.get_logger("transformers")
logger.info("INFO")
logger.warning("WARN")
class eval:
    def __init__(self):
        self.triples = list()
        self.pred = list()
        self.ref = list()
        self.bleu = evaluate.load("bleu")
        self.rouge = evaluate.load("rouge")
        self.meteor = evaluate.load("meteor")
        self.bertscore = evaluate.load("bertscore")
        # the id of template that have 2 entites in Lc_quad
        self.EntNum_d = [107, 407, 108, 408, 7, 307, 8, 308, 15, 315, 16, 316]


    # load from processed json and into the self.triples that could be sent to the models to run
    def load_sqb(self, convert=False):
        '''
        the self.triples would be a list of dictionary which has "value" (the first item is the original triple, the second is the converted relational triple)
        "answer", "question" and "additional_fct" (the relational fact)
        '''
        with open("/storage/yan/primekg/qg_dataset/SQB/Full_processed_sqb_dev.json") as f:
            triples = json.load(f)
            for triple in triples:
                # convert fact about relation into a triple, not used for GAIN setting, this is following the difficult controllable paper setting.
                if convert:
                    add_fact = [i.strip("] ") for i in triple["additional_fct"][:-1].split("[")][1:]
                    assert len(add_fact) == 3
                    triple["value"].append(add_fact)
                self.triples.append(triple)
        return

    def load_lcquad(self,):
        with open ("/storage/yan/primekg/qg_dataset/lc-quad/test.json", "r") as f:
            pair = json.load(f)
            out = dict()
            out["question"] = pair["corrected_question"]
            
        pass

    
    def load_webSP(self,):
        pass
    def run_GAIN(self, data = "sqb"):
        '''
        the answer will be masked, however this only applies to the cases for single triple
        # can only test on two node triple with tail as answer being masked setting
        '''
        if data == "sqb":
            values  = [i["value"][:1] for i in self.triples]

        prompts =  ["question generation: " + i[0][0] + '|' + i[0][1] for i in values]
        pipe = pipeline("text2text-generation", model="yhshu/GAIN-triple-question-generation", tokenizer="google-t5/t5-base", device="cuda")
        self.pred =   [i["generated_text"] for i in pipe(prompts, batch_size=30)]

        return self.pred 

    def run_inference(self, model = "bio_LLM", data = "sqb", baseline=True):
        self.ref  = [i["question"] for i in self.triples]
        if model == "bio_LLM":
            out = call_bioLLM(self.triples)
            print(f"files from eval file {out}")
            self.pred =  [i["generated_text"] for i in out]
        if model == "mistral":
            out = call_mistral(self.triples, baseline=baseline)[1:]
            self.pred = [i["generated_text"] for i in out]
        if model == "gpt":
            self.pred = [i["generated_text"] for i in call_openAI(self.triples)]
        self.pred = [self.pos_processing(sent) for sent in self.pred]
        if model == "GAIN":
            self.pred = self.run_GAIN()

        assert len(self.pred) == len(self.ref)
        print(f"{len(self.pred)} questions has been generated by {model} on dataset {data}.")
        return

    # helper function for mistral, gpt and bio-LLM for extracting the generated question from the generated text
    def pos_processing(self, ans):
        text = ans.split("Explanation")[0]
        text = text.split("Question")[-1]
        text = text.split("the question could be")[-1]
        text = text.split("the following question")[-1]
        text = text.split("?")[0] +"?"
        if "Wh" in text:
            text = "Wh" +text.split("Wh")[-1] 
        return text.strip(": \n")



    def save_report(self, model = "bioLLM", data = "sqb"):
        bertscore = self.bertscore.compute(predictions= self.pred, references =self.ref, model_type="distilbert-base-uncased")
        for index in bertscore:
            if not index == "hashcode":
                bertscore[index] = mean(bertscore[index])
        result = {"prediction":self.pred, "reference":self.ref, 
                    "bleu":self.bleu.compute(predictions=self.pred, references=self.ref), 
                    "rouge":self.rouge.compute(predictions = self.pred, references = self.ref)["rougeL"], 
                    "meteor":self.meteor.compute(predictions = self.pred, references = self.ref)["meteor"],
                    "bertscore":bertscore}
        with open(f"{model}_{data}_w_addFact.json", "w") as f:
            json.dump(result, f, indent=4)
        print(f"Evaluation of model {model} on dataset {data} has been saved in {model}_{data}_w_addFact.json.") 

    # for loading the answer from Lc_quad
    def get_answer(self, query):
        endpoint = "http://dbpedia.org/sparql"
        sparql = SPARQLWrapper(endpoint)
        sparql.setReturnFormat(JSON)
        
        sparql.setQuery(
            query
        )

        try:
            ret = sparql.queryAndConvert()

            for r in ret["results"]["bindings"]:
                if r["label"]["xml:lang"] == "en":
                    out = r["label"]["value"]
    #        return ret["results"]["bindings"]
        except Exception as e:
            print(e)
        time.sleep(2) 
        return out
    
    def test_(self):
        form_prompt_gpt(self.triples)

if __name__ == "__main__":
    model, data = "mistral", "sqb"
    self.test_()
    eval = eval()
    eval.load_sqb(convert=False)
    eval.run_inference(model, data)
    eval.save_report(model, data)