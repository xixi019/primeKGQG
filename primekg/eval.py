import evaluate
from transformers import pipeline
import json
from statistics import mean
from gpt_baseline_1 import call_openAI
from mistral.mistral_baseline import call_bioLLM, call_mistral
from util import form_prompt_gpt

class eval:
    def __init__(self):
        self.triples = list()
        self.pred = list()
        self.ref = list()
        self.bleu = evaluate.load("bleu")
        self.rouge = evaluate.load("rouge")
        self.meteor = evaluate.load("meteor")
        self.bertscore = evaluate.load("bertscore")

    # load from processed json and into the self.triples that could be sent to the models to run
    def load_sqb(self, ):
        '''
        the self.triples would be a list of dictionary which has "value" (the first item is the original triple, the second is the converted relational triple)
        "answer", "question" and "additional_fct" (the relational fact)
        '''
        with open("/storage/yan/primekg/qg_dataset/processed_sqb_dev.json") as f:
            triples = json.load(f)
            for triple in triples[:20]:
                # convert fact about relation into a triple, not used for GAIN setting, this is following the difficult controllable paper setting.
                add_fact = [i.strip("] ") for i in triple["additional_fct"][:-1].split("[")][1:]
                assert len(add_fact) == 3
                triple["value"].append(add_fact)
                self.triples.append(triple)
        return

    def load_webSP(self, ):
        pass

    def run_GAIN(self, data = "sqb"):
        '''
        the answer will be masked, however this only applies to the cases for single triple
        # can only test on two node triple with tail as answer being masked setting
        '''
        self.ref  = [i["question"] for i in self.triples]

        if data == "sqb":
            values  = [i["value"][:1] for i in self.triples]


        prompts =  ["question generation: " + i[0][0] + '|' + i[0][1] for i in values]
        pipe = pipeline("text2text-generation", model="yhshu/GAIN-triple-question-generation", tokenizer="google-t5/t5-base", device="cuda")
        self.pred =   [i["generated_text"] for i in pipe(prompts, batch_size=30)]
        assert len(self.pred) == len(self.ref)

        print(f"{len(self.pred)} questions has been generated by GAIN on dataset {data}.")
        return

    def run_bioLLM(self, data = "sqb"):
        self.ref  = [i["question"] for i in self.triples]
        self.pred =  [i["generated_text"] for i in call_bioLLM(self.triples)]
        assert len(self.pred) == len(self.ref)
        print(f"{len(self.pred)} questions has been generated by GAIN on dataset {data}.")
        return

    # helper function for mistral, gpt and bio-LLM for extracting the generated question from the generated text
    def pos_processing(self, ans):
        return ans.captilize()


    def save_report(self, model = "bioLLM", data = "sqb"):
        bertscore = self.bertscore.compute(predictions= self.pred, references =self.ref, model_type="distilbert-base-uncased")
        for index in bertscore:
            if not index == "hashcode":
                bertscore[index] = mean(bertscore[index])
        result = {"prediction":self.pred, "reference":self.ref, 
                    "bleu":self.bleu.compute(predictions=self.pred, references=self.ref), 
                    "rouge":self.rouge.compute(predictions = self.pred, references = self.ref)["rougeL"], 
                    "meteor":self.meteor.compute(predictions = self.pred, references = self.ref)["meteor"],
                    "bertscore":bertscore}
        with open(f"{model}_{data}.json", "w") as f:
            json.dump(result, f, indent=4)
        print(f"Evaluation of model {model} on dataset {data} has been saved in {model}_{data}.json.") 


if __name__ == "__main__":
    eval = eval()
    eval.load_sqb()
    eval.run_bioLLM()
    eval.save_report()